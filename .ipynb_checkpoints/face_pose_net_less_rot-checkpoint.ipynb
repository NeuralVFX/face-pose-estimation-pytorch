{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from util import losses as loss\n",
    "from util import loaders as load\n",
    "from models import networks as n\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "from onnx import optimizer\n",
    "\n",
    "\n",
    "def set_lr_sched( epochs, iters, mult):\n",
    "    # lr_schedule\n",
    "    mult_iter = iters\n",
    "    iter_stack = []\n",
    "    for a in range(epochs):\n",
    "        iter_stack += [math.cos((x / mult_iter) * 3.14) * .5 + .5 for x in (range(int(mult_iter)))]\n",
    "        mult_iter *= mult\n",
    "    return iter_stack\n",
    "\n",
    "def set_opt_lr(opt,lr):\n",
    "    opt.param_groups[0]['lr'] = lr[0]\n",
    "    opt.param_groups[1]['lr'] = lr[1]\n",
    "    opt.param_groups[2]['lr'] = lr[2]\n",
    "    \n",
    "def train(mod, opt, crit,crit_b, x, y,lr ,lr_lookup,current_iter):\n",
    "    lr_mult = lr_lookup[current_iter]\n",
    "    set_opt_lr(opt,lr*lr_mult)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    yhat = mod(x)\n",
    "    loss_a = crit(yhat,y.float())\n",
    "    acc = crit_b(yhat,y.float())\n",
    "    loss = loss_a + acc\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss_a, acc\n",
    "\n",
    "def test(mod, crit,crit_b, x, y):\n",
    "    yhat = mod(x)\n",
    "    loss = crit(yhat,y.float())\n",
    "    acc= crit_b(yhat,y.float())    \n",
    "    return loss, acc\n",
    "\n",
    "def one_run(freeze,lr_list,lr_array):\n",
    "    current_iter = 0\n",
    "    current_epoch = 0\n",
    "    done = False\n",
    "    res_net.set_freeze(freeze)\n",
    "    while not done:\n",
    "        epoch_test_loss = []\n",
    "        epoch_train_loss = []\n",
    "        epoch_test_acc = []\n",
    "        epoch_train_acc = []\n",
    "        \n",
    "        # TRAIN LOOP\n",
    "        res_net.train()\n",
    "        for x, y in mtrain:\n",
    "            \n",
    "            if current_iter > len(lr_list)-1:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "            x,y =x.cuda(),y.cuda()\n",
    "            train_loss, train_acc = train(res_net, adam_opt,mse_loss,point_loss, x, y, lr_array, lr_list, current_iter)\n",
    "\n",
    "            epoch_train_loss.append(torch.mean(train_loss).cpu().detach().numpy())\n",
    "            epoch_train_acc.append(torch.mean(train_acc.float()).cpu().detach().numpy())\n",
    "\n",
    "            current_iter +=1\n",
    "\n",
    "        # TEST LOOP\n",
    "        res_net.eval()\n",
    "        for x, y in mtest:\n",
    "            \n",
    "            x,y =x.cuda(),y.cuda()\n",
    "            test_loss, test_acc = test(res_net, mse_loss,point_loss, x, y)\n",
    "            epoch_test_loss.append(torch.mean(test_loss).cpu().detach().numpy())\n",
    "            epoch_test_acc.append(torch.mean(test_acc.float()).cpu().detach().numpy())\n",
    "        \n",
    "        print(f'train loss: {np.array(epoch_train_loss).mean()}   train acc: {np.array(epoch_train_acc).mean()}')\n",
    "        print(f'test loss: {np.array(epoch_test_loss).mean()}   test acc: {np.array(epoch_test_acc).mean()}')\n",
    "\n",
    "        train_loss_list.append(np.array(epoch_train_loss).mean())\n",
    "        test_loss_list.append(np.array(epoch_test_loss).mean())\n",
    "\n",
    "        current_epoch+=1\n",
    "    print ('Done')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import optimizer   \n",
    "\n",
    "def export_onnx(res_net,name):\n",
    "    output_names = [ \"output1\" ]\n",
    "    dummy_input = torch.zeros([1,3,96,96]).cuda()\n",
    "    res_net.eval()\n",
    "    res_net.train(False)\n",
    "    torch.onnx.export(res_net, dummy_input, name+\".onnx\", verbose=True, output_names=output_names)\n",
    "\n",
    "    model = onnx.load(name+'.onnx')\n",
    "\n",
    "\n",
    "    passes = [\"fuse_transpose_into_gemm\",\"fuse_bn_into_conv\",\"fuse_add_bias_into_conv\"]\n",
    "    optimized_model = optimizer.optimize(model, passes)\n",
    "    onnx.save(optimized_model, name+'_opt.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2cac7af2927a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mbar_chart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(8, 12))\n",
    "fig.patch.set_alpha(0.0)\n",
    "\n",
    "count = 0\n",
    "for idx in range(3):\n",
    "            a, b = next(iter(train_data))\n",
    "            ax[count, 0].cla()\n",
    "            bar_chart(ax[count, 0],b)\n",
    "            ax[count, 1].cla()\n",
    "            ax[count, 1].imshow(cv2.cvtColor(np.flip(train_data.get_zhe_face(b.unsqueeze(0).numpy()).astype(np.float32),1),cv2.COLOR_BGR2RGB),interpolation = 'lanczos')\n",
    "            count += 1\n",
    "            \n",
    "title_dict = {4: f\"Blendshapes\",\n",
    "                      5: f\"OpenCV Render\"\n",
    "}\n",
    "count = 0\n",
    "for a in ax.flat:\n",
    "            a.set_xticklabels('')\n",
    "            a.set_xticks([])\n",
    "            \n",
    "            a.tick_params(axis=\"y\", labelcolor=\"white\")\n",
    "            a.set_facecolor('k')\n",
    "\n",
    "            if count % 2 != 0:\n",
    "                a.set_yticklabels('')\n",
    "                a.set_yticks([])\n",
    "            \n",
    "            #a.set_aspect('equal')\n",
    "            if count in title_dict.keys():\n",
    "                a.text(0.5,-0.15, title_dict[count], size=17, ha=\"center\", transform=a.transAxes,color='white')\n",
    "            #    a.set_title(title_dict[count], fontsize=12,fontdict = { 'verticalalignment': 'top'})\n",
    "            count += 1\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from util import losses as loss\n",
    "from util import loaders as load\n",
    "from models import networks as n\n",
    "import torch.nn as nn\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# Network\n",
    "############################################################################\n",
    "\n",
    "res_net = n.CustomResnet(51)\n",
    "res_net.cuda()\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# Loss\n",
    "############################################################################\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "point_loss = loss.PointLoss().cuda()\n",
    "\n",
    "############################################################################\n",
    "# Optimizer\n",
    "############################################################################\n",
    "\n",
    "lr = 1e-3\n",
    "lr_array = np.array([lr/100,lr/10,lr])\n",
    "lr_groups = res_net.lr_groups()\n",
    "opt_params = [{\"params\":lr_groups[0].parameters(),\"lr\":lr_array[0]},\n",
    "              {\"params\":lr_groups[1].parameters(),\"lr\":lr_array[1]},\n",
    "              {\"params\":lr_groups[2].parameters(),\"lr\":lr_array[2]}]\n",
    "\n",
    "adam_opt = opt.Adam(opt_params,betas=(0.9, 0.999),weight_decay= .0001)\n",
    "\n",
    "############################################################################\n",
    "# Data Generators\n",
    "############################################################################\n",
    "\n",
    "transform = load.NormDenorm([.5, .5, .5], [.5, .5, .5])\n",
    "train_data= load.LandMarkDataset( transform, output_res=96, size = 10000, rand_rot = .7)\n",
    "test_data = load.LandMarkDataset( transform, output_res=96, size = 300, rand_rot = .7)   \n",
    "batch_size = 8\n",
    "mtrain = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "mtest  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=8, shuffle=False, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "test_loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #1\n",
      "Layer 0 : Grad:False\n",
      "Layer 1 : Grad:False\n",
      "Layer 2 : Grad:False\n",
      "Layer 3 : Grad:False\n",
      "Layer 4 : Grad:False\n",
      "Layer 5 : Grad:False\n",
      "train loss: 0.05263515189290047   train acc: 0.04900731146335602\n",
      "test loss: 0.049824755638837814   test acc: 0.04774409905076027\n",
      "Done\n",
      "Run #2\n",
      "Layer 0 : Grad:False\n",
      "Layer 1 : Grad:False\n",
      "Layer 2 : Grad:False\n",
      "Layer 3 : Grad:False\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fcf3d690f98>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.048157013952732086   train acc: 0.040925510227680206\n",
      "test loss: 0.04490002244710922   test acc: 0.03540730103850365\n",
      "Done\n",
      "Run #3\n",
      "Layer 0 : Grad:False\n",
      "Layer 1 : Grad:False\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.04277651011943817   train acc: 0.03324545547366142\n",
      "test loss: 0.038885053247213364   test acc: 0.02794649451971054\n",
      "Done\n",
      "Run #4\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.03846041485667229   train acc: 0.029711639508605003\n",
      "test loss: 0.037538792937994   test acc: 0.030345603823661804\n",
      "train loss: 0.03502105548977852   train acc: 0.026836203411221504\n",
      "test loss: 0.03197217360138893   test acc: 0.023072561249136925\n",
      "Done\n",
      "Run #5\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.03389713540673256   train acc: 0.026305874809622765\n",
      "test loss: 0.03526514396071434   test acc: 0.02808721363544464\n",
      "train loss: 0.032079774886369705   train acc: 0.024913903325796127\n",
      "test loss: 0.032959602773189545   test acc: 0.027919450774788857\n",
      "train loss: 0.031852081418037415   train acc: 0.025354795157909393\n",
      "test loss: 0.02930275723338127   test acc: 0.023178301751613617\n",
      "train loss: 0.028578301891684532   train acc: 0.021842775866389275\n",
      "test loss: 0.027164965867996216   test acc: 0.02087031677365303\n",
      "Done\n",
      "Run #6\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.0298931822180748   train acc: 0.023662908002734184\n",
      "test loss: 0.02961634285748005   test acc: 0.024516794830560684\n",
      "train loss: 0.02877325750887394   train acc: 0.02303834818303585\n",
      "test loss: 0.032231252640485764   test acc: 0.025083238258957863\n",
      "train loss: 0.029098134487867355   train acc: 0.023775994777679443\n",
      "test loss: 0.027200181037187576   test acc: 0.02133196033537388\n",
      "train loss: 0.026412032544612885   train acc: 0.021029073745012283\n",
      "test loss: 0.030791455879807472   test acc: 0.02346792072057724\n",
      "train loss: 0.02876099944114685   train acc: 0.02407805435359478\n",
      "test loss: 0.028473271057009697   test acc: 0.02360558696091175\n",
      "train loss: 0.02739088423550129   train acc: 0.022499334067106247\n",
      "test loss: 0.024921530857682228   test acc: 0.02050260826945305\n",
      "train loss: 0.024998478591442108   train acc: 0.02015511505305767\n",
      "test loss: 0.02256983332335949   test acc: 0.018344933167099953\n",
      "train loss: 0.024050455540418625   train acc: 0.018777403980493546\n",
      "test loss: 0.022816447541117668   test acc: 0.018324773758649826\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 1e-3  #### fixed  LR SETTING\n",
    "lr_array = np.array([lr/5,lr/3,lr])\n",
    "\n",
    "for i in range(1,7):\n",
    "    print (f'Run #{i}')\n",
    "    epoch_count = i\n",
    "    lr_list = set_lr_sched(epoch_count,mtrain.__len__()//batch_size,2.0)\n",
    "    freeze = (i-1)*2\n",
    "    if i == 4:\n",
    "        freeze = 100\n",
    "    one_run(freeze,lr_list,lr_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #1\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.023943472653627396   train acc: 0.018546806648373604\n",
      "test loss: 0.023277487605810165   test acc: 0.01809915341436863\n",
      "Done\n",
      "Run #2\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fcfc889e5c0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.024209732189774513   train acc: 0.018785253167152405\n",
      "test loss: 0.023701684549450874   test acc: 0.018243921920657158\n",
      "Done\n",
      "Run #3\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fcfc889e5c0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.023608742281794548   train acc: 0.018517784774303436\n",
      "test loss: 0.022773772478103638   test acc: 0.01822337880730629\n",
      "Done\n",
      "Run #4\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.023549726232886314   train acc: 0.018429245799779892\n",
      "test loss: 0.023066731169819832   test acc: 0.018671389669179916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fcfc889e5c0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.023238729685544968   train acc: 0.01824907772243023\n",
      "test loss: 0.020701907575130463   test acc: 0.01650410331785679\n",
      "Done\n",
      "Run #5\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.023158524185419083   train acc: 0.01828175224363804\n",
      "test loss: 0.022756066173315048   test acc: 0.017958756536245346\n",
      "train loss: 0.022974977269768715   train acc: 0.01802973635494709\n",
      "test loss: 0.022673729807138443   test acc: 0.018240438774228096\n",
      "train loss: 0.022933287546038628   train acc: 0.018117181956768036\n",
      "test loss: 0.021672876551747322   test acc: 0.017587294802069664\n",
      "train loss: 0.022634277120232582   train acc: 0.017708204686641693\n",
      "test loss: 0.021433040499687195   test acc: 0.016480552032589912\n",
      "Done\n",
      "Run #6\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.02276502549648285   train acc: 0.01787308230996132\n",
      "test loss: 0.021694466471672058   test acc: 0.017552535980939865\n",
      "train loss: 0.022517455741763115   train acc: 0.017725180834531784\n",
      "test loss: 0.022639136761426926   test acc: 0.017837995663285255\n",
      "train loss: 0.022404823452234268   train acc: 0.017633138224482536\n",
      "test loss: 0.02134680561721325   test acc: 0.016651257872581482\n",
      "train loss: 0.02218605950474739   train acc: 0.01745239645242691\n",
      "test loss: 0.022666608914732933   test acc: 0.017714308574795723\n",
      "train loss: 0.022365916520357132   train acc: 0.017827576026320457\n",
      "test loss: 0.02152002975344658   test acc: 0.017013931646943092\n",
      "train loss: 0.022091813385486603   train acc: 0.017556488513946533\n",
      "test loss: 0.021341893821954727   test acc: 0.01733972877264023\n",
      "train loss: 0.02172771841287613   train acc: 0.017125872895121574\n",
      "test loss: 0.021311648190021515   test acc: 0.01708158291876316\n",
      "train loss: 0.02174408547580242   train acc: 0.01707162708044052\n",
      "test loss: 0.020924273878335953   test acc: 0.01632530800998211\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 1e-4  #### fixed  LR SETTING\n",
    "lr_array = np.array([lr/5,lr/3,lr])\n",
    "\n",
    "for i in range(1,7):\n",
    "    print (f'Run #{i}')\n",
    "    epoch_count = i\n",
    "    lr_list = set_lr_sched(epoch_count,mtrain.__len__()//batch_size,2.0)\n",
    "\n",
    "    freeze = 100\n",
    "    one_run(freeze,lr_list,lr_array)\n",
    "\n",
    "#test loss: 0.019595347344875336   test acc: 0.01606038771569729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%0 : Float(1, 3, 96, 96)\n",
      "      %1 : Float(64, 3, 7, 7)\n",
      "      %2 : Float(64)\n",
      "      %3 : Float(64)\n",
      "      %4 : Float(64)\n",
      "      %5 : Float(64)\n",
      "      %6 : Long()\n",
      "      %7 : Float(64, 64, 3, 3)\n",
      "      %8 : Float(64)\n",
      "      %9 : Float(64)\n",
      "      %10 : Float(64)\n",
      "      %11 : Float(64)\n",
      "      %12 : Long()\n",
      "      %13 : Float(64, 64, 3, 3)\n",
      "      %14 : Float(64)\n",
      "      %15 : Float(64)\n",
      "      %16 : Float(64)\n",
      "      %17 : Float(64)\n",
      "      %18 : Long()\n",
      "      %19 : Float(64, 64, 3, 3)\n",
      "      %20 : Float(64)\n",
      "      %21 : Float(64)\n",
      "      %22 : Float(64)\n",
      "      %23 : Float(64)\n",
      "      %24 : Long()\n",
      "      %25 : Float(64, 64, 3, 3)\n",
      "      %26 : Float(64)\n",
      "      %27 : Float(64)\n",
      "      %28 : Float(64)\n",
      "      %29 : Float(64)\n",
      "      %30 : Long()\n",
      "      %31 : Float(64, 64, 3, 3)\n",
      "      %32 : Float(64)\n",
      "      %33 : Float(64)\n",
      "      %34 : Float(64)\n",
      "      %35 : Float(64)\n",
      "      %36 : Long()\n",
      "      %37 : Float(64, 64, 3, 3)\n",
      "      %38 : Float(64)\n",
      "      %39 : Float(64)\n",
      "      %40 : Float(64)\n",
      "      %41 : Float(64)\n",
      "      %42 : Long()\n",
      "      %43 : Float(128, 64, 3, 3)\n",
      "      %44 : Float(128)\n",
      "      %45 : Float(128)\n",
      "      %46 : Float(128)\n",
      "      %47 : Float(128)\n",
      "      %48 : Long()\n",
      "      %49 : Float(128, 128, 3, 3)\n",
      "      %50 : Float(128)\n",
      "      %51 : Float(128)\n",
      "      %52 : Float(128)\n",
      "      %53 : Float(128)\n",
      "      %54 : Long()\n",
      "      %55 : Float(128, 64, 1, 1)\n",
      "      %56 : Float(128)\n",
      "      %57 : Float(128)\n",
      "      %58 : Float(128)\n",
      "      %59 : Float(128)\n",
      "      %60 : Long()\n",
      "      %61 : Float(128, 128, 3, 3)\n",
      "      %62 : Float(128)\n",
      "      %63 : Float(128)\n",
      "      %64 : Float(128)\n",
      "      %65 : Float(128)\n",
      "      %66 : Long()\n",
      "      %67 : Float(128, 128, 3, 3)\n",
      "      %68 : Float(128)\n",
      "      %69 : Float(128)\n",
      "      %70 : Float(128)\n",
      "      %71 : Float(128)\n",
      "      %72 : Long()\n",
      "      %73 : Float(128, 128, 3, 3)\n",
      "      %74 : Float(128)\n",
      "      %75 : Float(128)\n",
      "      %76 : Float(128)\n",
      "      %77 : Float(128)\n",
      "      %78 : Long()\n",
      "      %79 : Float(128, 128, 3, 3)\n",
      "      %80 : Float(128)\n",
      "      %81 : Float(128)\n",
      "      %82 : Float(128)\n",
      "      %83 : Float(128)\n",
      "      %84 : Long()\n",
      "      %85 : Float(128, 128, 3, 3)\n",
      "      %86 : Float(128)\n",
      "      %87 : Float(128)\n",
      "      %88 : Float(128)\n",
      "      %89 : Float(128)\n",
      "      %90 : Long()\n",
      "      %91 : Float(128, 128, 3, 3)\n",
      "      %92 : Float(128)\n",
      "      %93 : Float(128)\n",
      "      %94 : Float(128)\n",
      "      %95 : Float(128)\n",
      "      %96 : Long()\n",
      "      %97 : Float(51, 512)\n",
      "      %98 : Float(51)) {\n",
      "  %99 : Float(1, 64, 48, 48) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2]](%0, %1), scope: CustomResnet/Sequential[resnet]/Conv2d[0]\n",
      "  %100 : Float(1, 64, 48, 48) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%99, %2, %3, %4, %5), scope: CustomResnet/Sequential[resnet]/BatchNorm2d[1]\n",
      "  %101 : Float(1, 64, 48, 48) = onnx::Relu(%100), scope: CustomResnet/Sequential[resnet]/ReLU[2]\n",
      "  %102 : Float(1, 64, 24, 24) = onnx::MaxPool[kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%101), scope: CustomResnet/Sequential[resnet]/MaxPool2d[3]\n",
      "  %103 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%102, %7), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %104 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%103, %8, %9, %10, %11), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %105 : Float(1, 64, 24, 24) = onnx::Relu(%104), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/ReLU[relu]\n",
      "  %106 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%105, %13), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %107 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%106, %14, %15, %16, %17), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %108 : Float(1, 64, 24, 24) = onnx::Add(%107, %102), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]\n",
      "  %109 : Float(1, 64, 24, 24) = onnx::Relu(%108), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/ReLU[relu]\n",
      "  %110 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%109, %19), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %111 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%110, %20, %21, %22, %23), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %112 : Float(1, 64, 24, 24) = onnx::Relu(%111), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/ReLU[relu]\n",
      "  %113 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%112, %25), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %114 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%113, %26, %27, %28, %29), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %115 : Float(1, 64, 24, 24) = onnx::Add(%114, %109), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]\n",
      "  %116 : Float(1, 64, 24, 24) = onnx::Relu(%115), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/ReLU[relu]\n",
      "  %117 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%116, %31), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/Conv2d[conv1]\n",
      "  %118 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%117, %32, %33, %34, %35), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/BatchNorm2d[bn1]\n",
      "  %119 : Float(1, 64, 24, 24) = onnx::Relu(%118), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/ReLU[relu]\n",
      "  %120 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%119, %37), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/Conv2d[conv2]\n",
      "  %121 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%120, %38, %39, %40, %41), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/BatchNorm2d[bn2]\n",
      "  %122 : Float(1, 64, 24, 24) = onnx::Add(%121, %116), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]\n",
      "  %123 : Float(1, 64, 24, 24) = onnx::Relu(%122), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/ReLU[relu]\n",
      "  %124 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%123, %43), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %125 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%124, %44, %45, %46, %47), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %126 : Float(1, 128, 12, 12) = onnx::Relu(%125), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/ReLU[relu]\n",
      "  %127 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%126, %49), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %128 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%127, %50, %51, %52, %53), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %129 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%123, %55), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n",
      "  %130 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%129, %56, %57, %58, %59), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n",
      "  %131 : Float(1, 128, 12, 12) = onnx::Add(%128, %130), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]\n",
      "  %132 : Float(1, 128, 12, 12) = onnx::Relu(%131), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/ReLU[relu]\n",
      "  %133 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%132, %61), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %134 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%133, %62, %63, %64, %65), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %135 : Float(1, 128, 12, 12) = onnx::Relu(%134), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/ReLU[relu]\n",
      "  %136 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%135, %67), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %137 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%136, %68, %69, %70, %71), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %138 : Float(1, 128, 12, 12) = onnx::Add(%137, %132), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]\n",
      "  %139 : Float(1, 128, 12, 12) = onnx::Relu(%138), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/ReLU[relu]\n",
      "  %140 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%139, %73), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/Conv2d[conv1]\n",
      "  %141 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%140, %74, %75, %76, %77), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/BatchNorm2d[bn1]\n",
      "  %142 : Float(1, 128, 12, 12) = onnx::Relu(%141), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/ReLU[relu]\n",
      "  %143 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%142, %79), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/Conv2d[conv2]\n",
      "  %144 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%143, %80, %81, %82, %83), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/BatchNorm2d[bn2]\n",
      "  %145 : Float(1, 128, 12, 12) = onnx::Add(%144, %139), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]\n",
      "  %146 : Float(1, 128, 12, 12) = onnx::Relu(%145), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/ReLU[relu]\n",
      "  %147 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%146, %85), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/Conv2d[conv1]\n",
      "  %148 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%147, %86, %87, %88, %89), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/BatchNorm2d[bn1]\n",
      "  %149 : Float(1, 128, 12, 12) = onnx::Relu(%148), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/ReLU[relu]\n",
      "  %150 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%149, %91), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/Conv2d[conv2]\n",
      "  %151 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%150, %92, %93, %94, %95), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/BatchNorm2d[bn2]\n",
      "  %152 : Float(1, 128, 12, 12) = onnx::Add(%151, %146), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]\n",
      "  %153 : Float(1, 128, 12, 12) = onnx::Relu(%152), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/ReLU[relu]\n",
      "  %154 : Dynamic = onnx::Pad[mode=constant, pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0](%153), scope: CustomResnet/ResHead[reshead]/AvgPool2d[av_pool]\n",
      "  %155 : Float(1, 128, 2, 2) = onnx::AveragePool[kernel_shape=[8, 8], pads=[0, 0, 0, 0], strides=[6, 6]](%154), scope: CustomResnet/ResHead[reshead]/AvgPool2d[av_pool]\n",
      "  %156 : Dynamic = onnx::Constant[value=  -1  512 [ CPULongTensor{2} ]](), scope: CustomResnet/ResHead[reshead]\n",
      "  %157 : Float(1, 512) = onnx::Reshape(%155, %156), scope: CustomResnet/ResHead[reshead]\n",
      "  %158 : Float(1, 51) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%157, %97, %98), scope: CustomResnet/ResHead[reshead]/Linear[linear]\n",
      "  %159 : Dynamic = onnx::Constant[value= -1  51 [ CPULongTensor{2} ]](), scope: CustomResnet\n",
      "  %output1 : Float(1, 51) = onnx::Reshape(%158, %159), scope: CustomResnet\n",
      "  return (%output1);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "export_onnx(res_net,'run_b')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #1\n",
      "Layer 0 : Grad:False\n",
      "Layer 1 : Grad:False\n",
      "Layer 2 : Grad:False\n",
      "Layer 3 : Grad:False\n",
      "Layer 4 : Grad:False\n",
      "Layer 5 : Grad:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fcf4a0dc278>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.026610655710101128   train acc: 0.021894782781600952\n",
      "test loss: 0.026444092392921448   test acc: 0.020223695784807205\n",
      "Done\n",
      "Run #2\n",
      "Layer 0 : Grad:False\n",
      "Layer 1 : Grad:False\n",
      "Layer 2 : Grad:False\n",
      "Layer 3 : Grad:False\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.025869350880384445   train acc: 0.021539244800806046\n",
      "test loss: 0.022303177043795586   test acc: 0.01828113943338394\n",
      "Done\n",
      "Run #3\n",
      "Layer 0 : Grad:False\n",
      "Layer 1 : Grad:False\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.025221211835741997   train acc: 0.021004797890782356\n",
      "test loss: 0.022675225511193275   test acc: 0.018006544560194016\n",
      "Done\n",
      "Run #4\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.025617383420467377   train acc: 0.021571215242147446\n",
      "test loss: 0.028717949986457825   test acc: 0.0241231769323349\n",
      "train loss: 0.024956200271844864   train acc: 0.02061409130692482\n",
      "test loss: 0.022339049726724625   test acc: 0.018067793920636177\n",
      "Done\n",
      "Run #5\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.025363333523273468   train acc: 0.021447600796818733\n",
      "test loss: 0.028334785252809525   test acc: 0.023366069421172142\n",
      "train loss: 0.024848710745573044   train acc: 0.02079237625002861\n",
      "test loss: 0.02943340502679348   test acc: 0.023048363626003265\n",
      "train loss: 0.025750359520316124   train acc: 0.02183915488421917\n",
      "test loss: 0.023023171350359917   test acc: 0.019982028752565384\n",
      "train loss: 0.02265101484954357   train acc: 0.018668247386813164\n",
      "test loss: 0.021750520914793015   test acc: 0.018026744946837425\n",
      "Done\n",
      "Run #6\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.024757595732808113   train acc: 0.0210658460855484\n",
      "test loss: 0.02690141461789608   test acc: 0.02240983210504055\n",
      "train loss: 0.024217601865530014   train acc: 0.020527053624391556\n",
      "test loss: 0.02714037336409092   test acc: 0.026285678148269653\n",
      "train loss: 0.025254616513848305   train acc: 0.021671857684850693\n",
      "test loss: 0.02256348356604576   test acc: 0.019430547952651978\n",
      "train loss: 0.022795282304286957   train acc: 0.018901633098721504\n",
      "test loss: 0.0263723935931921   test acc: 0.02410135604441166\n",
      "train loss: 0.025682324543595314   train acc: 0.022029289975762367\n",
      "test loss: 0.025057673454284668   test acc: 0.021279234439134598\n",
      "train loss: 0.023996373638510704   train acc: 0.020703716203570366\n",
      "test loss: 0.02256924845278263   test acc: 0.02008269727230072\n",
      "train loss: 0.022071823477745056   train acc: 0.0185774527490139\n",
      "test loss: 0.019577769562602043   test acc: 0.017050474882125854\n",
      "train loss: 0.02091793157160282   train acc: 0.017359530553221703\n",
      "test loss: 0.020325982943177223   test acc: 0.016827918589115143\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 1e-3  #### fixed  LR SETTING\n",
    "lr_array = np.array([lr/5,lr/3,lr])\n",
    "\n",
    "for i in range(1,7):\n",
    "    print (f'Run #{i}')\n",
    "    epoch_count = i\n",
    "    lr_list = set_lr_sched(epoch_count,mtrain.__len__()//batch_size,2.0)\n",
    "    freeze = (i-1)*2\n",
    "    if i == 4:\n",
    "        freeze = 100\n",
    "    one_run(freeze,lr_list,lr_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #1\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.020336704328656197   train acc: 0.01726873777806759\n",
      "test loss: 0.02102261781692505   test acc: 0.01728764921426773\n",
      "Done\n",
      "Run #2\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.020796410739421844   train acc: 0.017178509384393692\n",
      "test loss: 0.02000664733350277   test acc: 0.016957592219114304\n",
      "Done\n",
      "Run #3\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.020633572712540627   train acc: 0.017148897051811218\n",
      "test loss: 0.01916036196053028   test acc: 0.01652032695710659\n",
      "Done\n",
      "Run #4\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.020441671833395958   train acc: 0.017021765932440758\n",
      "test loss: 0.019725069403648376   test acc: 0.016454165801405907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fcfc88bfe80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/torch1cv/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.020345844328403473   train acc: 0.017037708312273026\n",
      "test loss: 0.018427660688757896   test acc: 0.0159874577075243\n",
      "Done\n",
      "Run #5\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.020442070439457893   train acc: 0.016898727044463158\n",
      "test loss: 0.019768211990594864   test acc: 0.016915351152420044\n",
      "train loss: 0.020205054432153702   train acc: 0.01687479205429554\n",
      "test loss: 0.01999376155436039   test acc: 0.016452733427286148\n",
      "train loss: 0.020117420703172684   train acc: 0.016958393156528473\n",
      "test loss: 0.019043639302253723   test acc: 0.016225755214691162\n",
      "train loss: 0.01968511939048767   train acc: 0.01662704348564148\n",
      "test loss: 0.01887953281402588   test acc: 0.01602904684841633\n",
      "Done\n",
      "Run #6\n",
      "Layer 0 : Grad:True\n",
      "Layer 1 : Grad:True\n",
      "Layer 2 : Grad:True\n",
      "Layer 3 : Grad:True\n",
      "Layer 4 : Grad:True\n",
      "Layer 5 : Grad:True\n",
      "train loss: 0.01981465518474579   train acc: 0.016690945252776146\n",
      "test loss: 0.019592847675085068   test acc: 0.016579942777752876\n",
      "train loss: 0.019651098176836967   train acc: 0.016479792073369026\n",
      "test loss: 0.019421443343162537   test acc: 0.01628776267170906\n",
      "train loss: 0.019885482266545296   train acc: 0.016731683164834976\n",
      "test loss: 0.01879253052175045   test acc: 0.015867987647652626\n",
      "train loss: 0.01942157745361328   train acc: 0.016247818246483803\n",
      "test loss: 0.018923427909612656   test acc: 0.01625272072851658\n",
      "train loss: 0.019483787938952446   train acc: 0.01655789464712143\n",
      "test loss: 0.018762532621622086   test acc: 0.016743451356887817\n",
      "train loss: 0.019378168508410454   train acc: 0.016321804374456406\n",
      "test loss: 0.01921417936682701   test acc: 0.016395604237914085\n",
      "train loss: 0.01912824623286724   train acc: 0.016099480912089348\n",
      "test loss: 0.018539538607001305   test acc: 0.015516801737248898\n",
      "train loss: 0.019260071218013763   train acc: 0.015993772074580193\n",
      "test loss: 0.01875890977680683   test acc: 0.01568632386624813\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 1e-4  #### fixed  LR SETTING\n",
    "lr_array = np.array([lr/5,lr/3,lr])\n",
    "\n",
    "for i in range(1,7):\n",
    "    print (f'Run #{i}')\n",
    "    epoch_count = i\n",
    "    lr_list = set_lr_sched(epoch_count,mtrain.__len__()//batch_size,2.0)\n",
    "\n",
    "    freeze = 100\n",
    "    one_run(freeze,lr_list,lr_array)\n",
    "\n",
    "torch.save(res_net.state_dict(), 'github_test_a.json')\n",
    "#test loss: 0.019595347344875336   test acc: 0.01606038771569729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%0 : Float(1, 3, 96, 96)\n",
      "      %1 : Float(64, 3, 7, 7)\n",
      "      %2 : Float(64)\n",
      "      %3 : Float(64)\n",
      "      %4 : Float(64)\n",
      "      %5 : Float(64)\n",
      "      %6 : Long()\n",
      "      %7 : Float(64, 64, 3, 3)\n",
      "      %8 : Float(64)\n",
      "      %9 : Float(64)\n",
      "      %10 : Float(64)\n",
      "      %11 : Float(64)\n",
      "      %12 : Long()\n",
      "      %13 : Float(64, 64, 3, 3)\n",
      "      %14 : Float(64)\n",
      "      %15 : Float(64)\n",
      "      %16 : Float(64)\n",
      "      %17 : Float(64)\n",
      "      %18 : Long()\n",
      "      %19 : Float(64, 64, 3, 3)\n",
      "      %20 : Float(64)\n",
      "      %21 : Float(64)\n",
      "      %22 : Float(64)\n",
      "      %23 : Float(64)\n",
      "      %24 : Long()\n",
      "      %25 : Float(64, 64, 3, 3)\n",
      "      %26 : Float(64)\n",
      "      %27 : Float(64)\n",
      "      %28 : Float(64)\n",
      "      %29 : Float(64)\n",
      "      %30 : Long()\n",
      "      %31 : Float(64, 64, 3, 3)\n",
      "      %32 : Float(64)\n",
      "      %33 : Float(64)\n",
      "      %34 : Float(64)\n",
      "      %35 : Float(64)\n",
      "      %36 : Long()\n",
      "      %37 : Float(64, 64, 3, 3)\n",
      "      %38 : Float(64)\n",
      "      %39 : Float(64)\n",
      "      %40 : Float(64)\n",
      "      %41 : Float(64)\n",
      "      %42 : Long()\n",
      "      %43 : Float(128, 64, 3, 3)\n",
      "      %44 : Float(128)\n",
      "      %45 : Float(128)\n",
      "      %46 : Float(128)\n",
      "      %47 : Float(128)\n",
      "      %48 : Long()\n",
      "      %49 : Float(128, 128, 3, 3)\n",
      "      %50 : Float(128)\n",
      "      %51 : Float(128)\n",
      "      %52 : Float(128)\n",
      "      %53 : Float(128)\n",
      "      %54 : Long()\n",
      "      %55 : Float(128, 64, 1, 1)\n",
      "      %56 : Float(128)\n",
      "      %57 : Float(128)\n",
      "      %58 : Float(128)\n",
      "      %59 : Float(128)\n",
      "      %60 : Long()\n",
      "      %61 : Float(128, 128, 3, 3)\n",
      "      %62 : Float(128)\n",
      "      %63 : Float(128)\n",
      "      %64 : Float(128)\n",
      "      %65 : Float(128)\n",
      "      %66 : Long()\n",
      "      %67 : Float(128, 128, 3, 3)\n",
      "      %68 : Float(128)\n",
      "      %69 : Float(128)\n",
      "      %70 : Float(128)\n",
      "      %71 : Float(128)\n",
      "      %72 : Long()\n",
      "      %73 : Float(128, 128, 3, 3)\n",
      "      %74 : Float(128)\n",
      "      %75 : Float(128)\n",
      "      %76 : Float(128)\n",
      "      %77 : Float(128)\n",
      "      %78 : Long()\n",
      "      %79 : Float(128, 128, 3, 3)\n",
      "      %80 : Float(128)\n",
      "      %81 : Float(128)\n",
      "      %82 : Float(128)\n",
      "      %83 : Float(128)\n",
      "      %84 : Long()\n",
      "      %85 : Float(128, 128, 3, 3)\n",
      "      %86 : Float(128)\n",
      "      %87 : Float(128)\n",
      "      %88 : Float(128)\n",
      "      %89 : Float(128)\n",
      "      %90 : Long()\n",
      "      %91 : Float(128, 128, 3, 3)\n",
      "      %92 : Float(128)\n",
      "      %93 : Float(128)\n",
      "      %94 : Float(128)\n",
      "      %95 : Float(128)\n",
      "      %96 : Long()\n",
      "      %97 : Float(51, 512)\n",
      "      %98 : Float(51)) {\n",
      "  %99 : Float(1, 64, 48, 48) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2]](%0, %1), scope: CustomResnet/Sequential[resnet]/Conv2d[0]\n",
      "  %100 : Float(1, 64, 48, 48) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%99, %2, %3, %4, %5), scope: CustomResnet/Sequential[resnet]/BatchNorm2d[1]\n",
      "  %101 : Float(1, 64, 48, 48) = onnx::Relu(%100), scope: CustomResnet/Sequential[resnet]/ReLU[2]\n",
      "  %102 : Float(1, 64, 24, 24) = onnx::MaxPool[kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%101), scope: CustomResnet/Sequential[resnet]/MaxPool2d[3]\n",
      "  %103 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%102, %7), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %104 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%103, %8, %9, %10, %11), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %105 : Float(1, 64, 24, 24) = onnx::Relu(%104), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/ReLU[relu]\n",
      "  %106 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%105, %13), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %107 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%106, %14, %15, %16, %17), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %108 : Float(1, 64, 24, 24) = onnx::Add(%107, %102), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]\n",
      "  %109 : Float(1, 64, 24, 24) = onnx::Relu(%108), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[0]/ReLU[relu]\n",
      "  %110 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%109, %19), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %111 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%110, %20, %21, %22, %23), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %112 : Float(1, 64, 24, 24) = onnx::Relu(%111), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/ReLU[relu]\n",
      "  %113 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%112, %25), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %114 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%113, %26, %27, %28, %29), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %115 : Float(1, 64, 24, 24) = onnx::Add(%114, %109), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]\n",
      "  %116 : Float(1, 64, 24, 24) = onnx::Relu(%115), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[1]/ReLU[relu]\n",
      "  %117 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%116, %31), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/Conv2d[conv1]\n",
      "  %118 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%117, %32, %33, %34, %35), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/BatchNorm2d[bn1]\n",
      "  %119 : Float(1, 64, 24, 24) = onnx::Relu(%118), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/ReLU[relu]\n",
      "  %120 : Float(1, 64, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%119, %37), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/Conv2d[conv2]\n",
      "  %121 : Float(1, 64, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%120, %38, %39, %40, %41), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/BatchNorm2d[bn2]\n",
      "  %122 : Float(1, 64, 24, 24) = onnx::Add(%121, %116), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]\n",
      "  %123 : Float(1, 64, 24, 24) = onnx::Relu(%122), scope: CustomResnet/Sequential[resnet]/Sequential[4]/BasicBlock[2]/ReLU[relu]\n",
      "  %124 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%123, %43), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %125 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%124, %44, %45, %46, %47), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %126 : Float(1, 128, 12, 12) = onnx::Relu(%125), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/ReLU[relu]\n",
      "  %127 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%126, %49), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %128 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%127, %50, %51, %52, %53), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %129 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%123, %55), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n",
      "  %130 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%129, %56, %57, %58, %59), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n",
      "  %131 : Float(1, 128, 12, 12) = onnx::Add(%128, %130), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]\n",
      "  %132 : Float(1, 128, 12, 12) = onnx::Relu(%131), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[0]/ReLU[relu]\n",
      "  %133 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%132, %61), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %134 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%133, %62, %63, %64, %65), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %135 : Float(1, 128, 12, 12) = onnx::Relu(%134), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/ReLU[relu]\n",
      "  %136 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%135, %67), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %137 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%136, %68, %69, %70, %71), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %138 : Float(1, 128, 12, 12) = onnx::Add(%137, %132), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]\n",
      "  %139 : Float(1, 128, 12, 12) = onnx::Relu(%138), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[1]/ReLU[relu]\n",
      "  %140 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%139, %73), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/Conv2d[conv1]\n",
      "  %141 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%140, %74, %75, %76, %77), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/BatchNorm2d[bn1]\n",
      "  %142 : Float(1, 128, 12, 12) = onnx::Relu(%141), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/ReLU[relu]\n",
      "  %143 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%142, %79), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/Conv2d[conv2]\n",
      "  %144 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%143, %80, %81, %82, %83), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/BatchNorm2d[bn2]\n",
      "  %145 : Float(1, 128, 12, 12) = onnx::Add(%144, %139), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]\n",
      "  %146 : Float(1, 128, 12, 12) = onnx::Relu(%145), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[2]/ReLU[relu]\n",
      "  %147 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%146, %85), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/Conv2d[conv1]\n",
      "  %148 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%147, %86, %87, %88, %89), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/BatchNorm2d[bn1]\n",
      "  %149 : Float(1, 128, 12, 12) = onnx::Relu(%148), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/ReLU[relu]\n",
      "  %150 : Float(1, 128, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%149, %91), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/Conv2d[conv2]\n",
      "  %151 : Float(1, 128, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, is_test=1, momentum=1](%150, %92, %93, %94, %95), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/BatchNorm2d[bn2]\n",
      "  %152 : Float(1, 128, 12, 12) = onnx::Add(%151, %146), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]\n",
      "  %153 : Float(1, 128, 12, 12) = onnx::Relu(%152), scope: CustomResnet/Sequential[resnet]/Sequential[5]/BasicBlock[3]/ReLU[relu]\n",
      "  %154 : Dynamic = onnx::Pad[mode=constant, pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0](%153), scope: CustomResnet/ResHead[reshead]/AvgPool2d[av_pool]\n",
      "  %155 : Float(1, 128, 2, 2) = onnx::AveragePool[kernel_shape=[8, 8], pads=[0, 0, 0, 0], strides=[6, 6]](%154), scope: CustomResnet/ResHead[reshead]/AvgPool2d[av_pool]\n",
      "  %156 : Dynamic = onnx::Constant[value=  -1  512 [ CPULongTensor{2} ]](), scope: CustomResnet/ResHead[reshead]\n",
      "  %157 : Float(1, 512) = onnx::Reshape(%155, %156), scope: CustomResnet/ResHead[reshead]\n",
      "  %158 : Float(1, 51) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%157, %97, %98), scope: CustomResnet/ResHead[reshead]/Linear[linear]\n",
      "  %159 : Dynamic = onnx::Constant[value= -1  51 [ CPULongTensor{2} ]](), scope: CustomResnet\n",
      "  %output1 : Float(1, 51) = onnx::Reshape(%158, %159), scope: CustomResnet\n",
      "  return (%output1);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# THIS ONE IS GOOD ENOUGH #\n",
    "export_onnx(res_net,'run_c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
